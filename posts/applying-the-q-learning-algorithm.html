<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../styles.css">
  <title>Applying the Q-Learning Algorithm</title>
  <script src="script.js" async="true"></script>
</head>

<body class="layout title-page">

  <!-- right side logo -->
  <!-- <div class="right-text title">
        small<br>data<br>by<br>dami.
    </div>
    <div class="rect"></div> -->

  <!-- left side nav bar -->
  <div class="left-nav">
    <div id="posts_home">
      <img src="icons/white arrow.png" class="arrow" alt="left arrow" id="arrow_p_home">
      <a href="../index.html">
        <div class="bflex" id="posts_home">POSTS</div>
      </a>
    </div>
    <p id="point">‚Ä¢</p>
    <div id="posts_about">
      <img src="icons/white arrow.png" class="arrow" alt="left arrow" id="arrow_p_about">
      <a href="../about.html">
        <div class="bflex">BIO</div>
      </a>
    </div>
  </div>

  <!-- body of post -->
  <div class="post-body">
    <img src="/pics/projectile_img.png" alt="alt pic" class="start-pic">
    
      <p>The code used for both parts of this project can be found <a href="https://github.com/akin-oladejo/projectile-env">here</a></p>
      <h1>Applying the Q-Learning Algorithm to a custom environment</h1>
      <p>In the previous article, we built the standard Q-learning algorithm from scratch and implemented it as a
        class, <code>Q</code>. In this article we will construct a simple environment and attempt to solve it
        using the <code>Q</code> class.</p>

      <h2>The Environment</h2>
      <p>A device shoots a projectile from the ground toward a target that is also at ground level. The shooter is
        initialized with a default initial velocity (we'll set this to 15m/s but you can change it if you want).
        Assuming acceleration due to gravity (g) is 9.8m/s¬≤, given a discrete target distance, determine the nearest
        <ins>available</ins> angle (in degrees) the projectile should be jetted at.
      </p>

      <p><b>Note</b>: We also supply the target range. The standard q-learning algorithm is commonly applied to
        discrete state and action spaces so it is more intuitive to provide whole number target values (and thus
        whole number states)</p>

      <p>Also, in this environment, there are nine angles available: 5¬∞, 10¬∞, 15¬∞, 20¬∞, 25¬∞, 30¬∞, 35¬∞, 40¬∞ and 45¬∞</p>

      <p>How might we solve this task?</p>

      <h2>Designing the state and action space</h2>
      <p>The environment itself will be implemented as a subclass of <code>Env</code> provided by the openai gym
        library. The <a href="https://www.gymlibrary.dev/content/basic_usage/">gym</a> library is a popular resource
        used to write environments in code for agents to interact with. It provides popular environments for testing
        algorithms and gives you the tools to create your custom environments. </p>

      <p>The advantage of subclassing <code>Env</code> is that by only overriding only the necessary methods, we can
        still solve our custom environment using baseline algorithms from other libraries designed with the
        <code>gym.Env</code> class in mind.
      </p>

      <h3>The State space</h3>
      <p>This is a simple environment. The states will simply be discrete values representing the target distances.
        Thus, I use the <code>Box</code> data structure provided in the <code>gym.spaces</code> sublibrary.
        <code>Box</code> is essentially an array that allows you to specify the lower and upper bounds for each
        dimension, as well as the shape and data type of the array. To understand this data structure better,
        consult the <a href="https://www.gymlibrary.dev/api/spaces/#box">documentation</a>. The documentation is
        also a good starting point to get familiar with other spaces provided in the <code>gym</code> universe.
      </p>

      <p>The state space is defined in the <code>__init__</code> method below. Look for the line:
  <pre>self.observation_space = Box(low=target_low, high=target_high, shape=(1,), dtype=np.int32)</pre>
      <p>The <code>__init__</code> method, a <a
          href="https://docs.python.org/3/reference/datamodel.html#special-method-names">dunder</a> (or magic
        method, as special methods like it are called in Python), is a special method that is called when an object
        of a class is instantiated. By passing the values <code>target_low</code> and <code>target_high</code> as
        instantiation parameters to the environment object, you set the bounds of the state space. By default, the
        default values are 3 and 22, meaning the target values to learn the angles for range from 3m to 22m.</p>

      <h3>The Action space</h3>
      <p>Similarly, the action space will be discrete, representing the angles to choose from. The action space will
        be represented by the <code>Discrete</code> data structure, which is simply a finite array that begins at 0
        and ends at the provided argument for `n`. There are a couple differences between <code>Discrete</code> and
        <code>Box</code> such as that <code>Discrete</code> strictly contains integer values, is of dimension 1 and
        starts at 0. <code>Box</code> on the other hand can handle multiple data types, can be multidimensional and
        allows you to set different bound values for each dimension.
      </p>

      <p><b>Note</b>: I used the <code>Box</code> data structure for the state space because I wanted it to be
        possible to have a non-zero lower target range i.e the user can determine that the lower target range is
        100m.</p>

      <p>The documentation for <code>Discrete</code> can be found <a
          href="https://www.gymlibrary.dev/api/spaces/#discrete">here</a></p>

      I set up the action space using these lines in the <code>__init__</code> method:
<pre>
self.action_space = Discrete(9)

self.action_to_angle = {
    0:5,
    1:10,
    2:15,
    3:20,
    4:25, 
    5:30, 
    6:35, 
    7:40,
    8:45
  }
</pre>
      The attribute <code>action_to_angle</code> will serve as a lookup for converting supplied action values to angles.
      <h2>The reward scheme</h2>
      <P>Since Q-learning is a value-based algorithm and obtains its policy indirectly using an action-value table, we
        need to come up with a scheme to encourage choosing the right angle. </P>
      <p>There are maybe better ways to do this, but I decided to go with this scheme:
      <ul>
        <li>Calculate error as the absolute distance between the target distance and the range covered by the
          projectile</li>
        <li>Create a dictionary to store the lowest errors attained at each supplied target value</li>
        <li>If the angle supplied during a timestep results in reducing the minimum error achieved for that
          target/state, the reward for that timestep is <code>+1</code>. If the error is greater, the reward is
          <code>-1</code>. If
          there is no change in the minimum error for that target value, the reward is simple, <code>0</code>.üòê
        </li>
      </ul>
      </p>

      <p>This way, the policy indirectly trained will be to either reduce the error, or supply the angle that achieved
        the lowest error.</p>


      <p>Having explained the design of this environment, let's get into the code.</p>
      <h2>Importing libraries</h2>
      <pre>
from gym import Env
import numpy as np
from gym.spaces import Discrete, Box
import matplotlib.pyplot as plt
%matplotlib inline
import random</pre>

      <h2>Initialization</h2>
      <p>Much of the initialization code has been explained above.</p>

      <p>In addition, an attribute <code>max_repeat</code> is created. <code>max_repeat</code> represents the maximum
        number of times a target value will be repeated when <code>step</code> is called. If <code>max_repeat</code>
        is set to 4 and the current target is 5m, you will need to call the <code>step</code> method 4 times to move
        on to the next target value. This allows us perform more Q-Table updates for each target during an episode.
      </p>

      <p>A dictionary <code>min_error</code> is created to store the minimum absolute errors attained for each target.
      </p>

      <p>Also, a dict <code>target_to_state</code> is created which will be used to convert the target distance to a
        state value, for the Q-Table.</p>
      <pre>
def __init__(self, initial_velocity=15, g=9.8, target_low=3, target_high=22, max_repeat=9):
  self.name = 'Projectile'
  <span class="comment"># define environment bounds</span>
  self.observation_space = Box(low=target_low, high=target_high, shape=(1,), dtype=np.int32)

  <span class="comment"># there are 9 actions corresponding to the interval of angles from 5 to 45</span>
  self.action_space = Discrete(9)

  <span class="comment"># store these for easy access later</span>
  self.n_actions = self.action_space.n
  self.n_states = target_high - target_low + 1

  self.v = initial_velocity <span class="comment"># the default value for initial_velocity is 15m/s</span>
  self.g = g <span class="comment"># the default value for acceleration due to gravity is 9.8m/s¬≤</span>
  
  <span class="comment"># the number of times to repeat a target value before moving on to the next</span>
  self.max_repeat = max_repeat 

  <span class="comment"># for action-to-angle conversion</span>
  self.action_to_angle = {
      0:5,
      1:10,
      2:15,
      3:20,
      4:25, 
      5:30, 
      6:35, 
      7:40,
      8:45
  }

  <span class="comment"># create a lookup of discrete state values corresponding to target distances</span>
  self.target_to_state = {i:x for x,i in enumerate((range(target_low, target_high+1)))}

  <span class="comment"># keep track of the lowest error attained for each target</span>
  self.min_error = {x:np.inf for x in range(self.observation_space.low[0], self.observation_space.high[0] + 1)}
  </pre>
      <h2>Reset</h2>
      <p>The <code>reset()</code> method is called to reset the state of an environment. Our <code>reset()</code>
        method
      <ol>
        <li>creates an attribute <code>target_list</code> which is an array of all the target distances and shuffles
          the elements. It does this so we can keep track of the current episode's arrangement of target values.
        </li>
        <li>selects an initial target and removes it from <code>target_list</code>, using the
          <code>new_target()</code> method
        </li>
        <li>initializes the range of the projectile to 0, because we are yet to take an action</li>
        <li>initializes an attribute <code>current_countdown</code> for that target. <code>current_countdown</code>
          starts at the value of <code>max_repeat</code> and is reduced at each time step by 1. When it eventually
          becomes 0, a new target will be supplied</li>
        <li>sets the <code>done</code> attribute to False. <code>done</code> becomes true when the episode is over
        </li>
        <li>returns the current observation using <code>self.get_obs()</code> and the current error calculated by
          <code>self.get_info()</code>
        </li>
      </ol>
      </p>
<pre>
def new_target(self):
  <span class="comment"># pick a target from the list of targets and remove it </span>
  self.target = self.target_list.pop()
    

def get_obs(self, verbose=False):
    <span class="comment"># self.target = self.observation_space.sample() # generate random target</span>
    self.state = self.target_to_state[self.target] <span class="comment"># represent target as state</span>

    <span class="comment"># if verbose=True, return the true value of the target distance, else return </span>
    <span class="comment"># its representation as a state value</span>
    if verbose:
        return f'Target distance: {self.target}m'
    else:
        return self.state


def get_info(self, verbose=True):
    <span class="comment"># return the number of meters by which we've missed the target</span>
    self.error = np.abs(self.target - self.range) <span class="comment"># absolute error</span>

    <span class="comment"># if verbose=True, return a full sentence, else just return the value of the error</span>
    if verbose:
        return f'Error: {self.error:.2f}m'
    else:
        return self.error

def reset(self, verbose=True):
    <span class="comment"># store the set of targets we can select from </span>
    self.target_list = list(range(self.observation_space.low[0], self.observation_space.high[0] + 1))

    random.shuffle(self.target_list) <span class="comment"># shuffle the available targets</span>

    self.new_target() <span class="comment"># generate a target value</span>
    
    self.range = 0 <span class="comment"># the projectile is yet to be shot, so range is still 0</span>
    self.current_countdown = self.repeat_count <span class="comment"># start the countdown for the current target</span>
    self.done = False

    return self.get_obs(verbose), self.get_info()
  </pre>


      <h2>Calculating the Range</h2>
      <p>The range (also known as horizontal distance) of the projectile is calculated using the standard formula for
        a projectile's range, assuming no air resistance and that the final height of the projectile is equal to its
        initial height:
        <img src="../pics/projectile range formula.png" alt="Formula for the range of a projectile">
        The angle, Œ∏, is converted to radians from degrees.
      </p>


<pre>
def calc_range(self):
    <span class="comment"># calculate the range of the projectile using the relation v¬≤¬∑sin(2Œ∏)/g</span>
    rad = np.pi/180 * angle
    range = (v**2) * np.sin(2 * rad) / g
    return range
</pre>

    <h2>Step</h2>
    <p>Calling <code>step</code> decrements <code>current_countdown</code> by 1. It takes an action value and
      calculates the range by calling <code>calc_range()</code>. The error is computed using
      <code>get_info()</code>.
    </p>
    <p>The reward is calculated using the scheme above. If the <code>current_countdown</code> is 0, a new target
      value is generated using <code>new_target</code> and <code>current_countdown</code> is reinitialized to the
      value of <code>max_repeat</code>. Once we run out of target values, <code>done</code> is set to
      <code>True</code>, ending the episode.
    </p>


<pre>
def step(self, action):
    <span class="comment"># print(target_list)</span>
    <span class="comment"># reduce the target repeat countdown by 1</span>
    current_countdown -= 1

    <span class="comment"># map the action to the angle of projectile</span>
    angle = action_to_angle[action]

    <span class="comment"># this is the range covered by the projectile at the given angle, save it in the </span>
    <span class="comment"># lookup table for current ranges</span>
    range = calc_range()

    <span class="comment"># evaluate error for this round</span>
    error = get_info(verbose=False) 

    <span class="comment"># if the error in this round is less than the history of errors for this round, the </span>
    <span class="comment"># reward is set to +1. If it is higher, -1. Else, the reward is 0</span>
    if error < min_error[target]:
        reward = 1 
        min_error[target] = error <span class="comment"># update error range value</span>
    elif error > min_error[target]:
        reward = -1
    elif error == min_error[target]:
        reward = 0

    if current_countdown == 0:
        if len(target_list) == 0:
            <span class="comment"># set done to True when there are no values left in target_list and end episode</span>
            done = True
        else:
            <span class="comment"># else supply a new target and start the repeat counter again</span>
            new_target()
            current_countdown = max_repeat
    
    return get_obs(), reward, done, get_info()</pre>


      <h2>Rendering</h2>
      <code>render()</code> contains the logic to visualize the target, range and error, using the <code>numpy</code>
      library to calculate displacement values and <code>matplotlib</code> library for visualization.
      
      
<pre>
def render(target, angle):
    v, g = 15, 9.8 <span class="comment"># initialize velocity and acceleration due to gravity</span>
    rad = np.pi/180 * angle
    plt.figure()
    params = {'mathtext.default': 'regular' }  <span class="comment"># mathtext, for subscripting 0 in v0 in plot title</span>
    tmax = (2 * v) * np.sin(rad) / g <span class="comment"># calculate time of flight</span>
    t = tmax*np.linspace(0,1,100) <span class="comment"># divide time of flight into 100 uniform time steps</span>
    x = ((v * t) * np.cos(rad)) <span class="comment"># horizontal distance at each time step</span>
    y = ((v * t) * np.sin(rad)) - ((0.5 * g) * (t ** 2)) <span class="comment"># vertical distance </span>

    plt.plot(x, y, color='g') <span class="comment"># plot path</span>

    <span class="comment"># draw line to target target (i.e desired distance) saved in `target`</span>
    plt.axvline(x = target, ls='--', color = 'b', label = f'target: {target}m') 

    <span class="comment"># draw projectile at final coordinates</span>
    plt.scatter(x[-1], y[-1], color='r', markerinitial_velocity="^", s=200)

    plt.ylim([0,10])
    plt.xlim(left=0)
    plt.title(f'$v_{0}$ = {v}m/s, Œ∏ = {angle}¬∞, abs. error = {np.abs(x[-1]-target):.2f}m')
    plt.legend()</pre>


<pre>render(target=15, angle=30)</pre>

      <img src="../pics/projectile plot.png" alt="Projectile plot">
      <h2>Putting it all together in a class: <code>Projectile</code></h2>

<pre>
class Projectile(Env):
    def __init__(self, initial_velocity=15, g=9.8, target_low=3, target_high=22, max_repeat=9):
        self.name = 'Projectile'
        <span class="comment"># define environment bounds</span>
        self.observation_space = Box(low=target_low, high=target_high, shape=(1,), dtype=np.int32)

        <span class="comment"># there are 9 actions corresponding to the interval of angles from 5 to 45</span>
        self.action_space = Discrete(9)

        <span class="comment"># store these for easy access later</span>
        self.n_actions = self.action_space.n
        self.n_states = target_high - target_low + 1

        self.v = initial_velocity <span class="comment"># the default value for initial_velocity is 15m/s</span>
        self.g = g <span class="comment"># the default value for acceleration due to gravity is 9.8m/s¬≤</span>
        
        <span class="comment"># the number of times to repeat a target value before moving on to the next</span>
        self.max_repeat = max_repeat 

        <span class="comment"># lookup the angle values for each action</span>
        self.action_to_angle = {
            0:5,
            1:10,
            2:15,
            3:20,
            4:25, 
            5:30, 
            6:35, 
            7:40,
            8:45
        }

        <span class="comment"># create a lookup of discrete state values corresponding to target distances</span>
        self.target_to_state = {i:x for x,i in enumerate((range(target_low, target_high+1)))}

        <span class="comment"># keep track of the lowest error attained for each target</span>
        self.min_error = {x:np.inf for x in range(self.observation_space.low[0], self.observation_space.high[0] + 1)} 


    def new_target(self):
        <span class="comment"># pick a target from the list of targets and remove it </span>
        self.target = self.target_list.pop()
        

    def get_obs(self, verbose=False):
        <span class="comment"># self.target = self.observation_space.sample() # generate random target</span>
        self.state = self.target_to_state[self.target] <span class="comment"># represent target as state</span>

        <span class="comment"># if verbose=True, return the true value of the target distance, else return </span>
        <span class="comment"># its representation as a state value</span>
        if verbose:
            return f'Target distance: {self.target}m'
        else:
            return self.state


    def get_info(self, verbose=True):
        <span class="comment"># return the number of meters by which we've missed the target</span>
        self.error = np.abs(self.target - self.range) <span class="comment"># absolute error</span>

        <span class="comment"># if verbose=True, return a full sentence, else just return the value of the error</span>
        if verbose:
            return f'Error: {self.error:.2f}m'
        else:
            return self.error

    def reset(self, verbose=True):
        <span class="comment"># store the set of targets we can select from </span>
        self.target_list = list(range(self.observation_space.low[0], self.observation_space.high[0] + 1))

        random.shuffle(self.target_list) <span class="comment"># shuffle the available targets</span>
        self.new_target() <span class="comment"># generate a target value</span>
        self.range = 0 <span class="comment"># the projectile is yet to be shot, so range is still 0</span>
        self.current_countdown = self.max_repeat <span class="comment"># start the countdown for the current target</span>
        self.done = False

        return self.get_obs(verbose), self.get_info()


    def calc_range(self):
        <span class="comment"># calculate the range of the projectile using the relation v¬≤¬∑sin(2Œ∏)/g</span>
        rad = np.pi/180 * self.angle
        range = (self.v**2) * np.sin(2 * rad) / self.g
        return range

    def step(self, action):
        <span class="comment"># print(self.target_list)</span>
        <span class="comment"># reduce the target repeat countdown by 1</span>
        self.current_countdown -= 1

        <span class="comment"># map the action to the angle of projectile</span>
        self.angle = self.action_to_angle[action]

        <span class="comment"># this is the range covered by the projectile at the given angle, save it in the </span>
        <span class="comment"># lookup table for current ranges</span>
        self.range = self.calc_range()

        <span class="comment"># evaluate error for this round</span>
        self.error = self.get_info(verbose=False) 

        <span class="comment"># if the error in this round is less than the history of errors for this round, the </span>
        <span class="comment"># reward is set to +1. If it is higher, -1. Else, the reward is 0</span>
        if self.error < self.min_error[self.target]:
            self.reward = 1 
            self.min_error[self.target] = self.error <span class="comment"># update error range value</span>
        elif self.error > self.min_error[self.target]:
            self.reward = -1
        elif self.error == self.min_error[self.target]:
            self.reward = 0

        if self.current_countdown == 0:
            if len(self.target_list) == 0:
                <span class="comment"># set self.done to True when there are no values left in self.target_list and end episode</span>
                self.done = True
            else:
                <span class="comment"># else supply a new target and start the repeat counter again</span>
                self.new_target()
                self.current_countdown = self.max_repeat
        
        return self.get_obs(), self.reward, self.done, self.get_info()
        

    def render(self):
        rad = np.pi/180 * self.angle
        plt.figure()
        <span class="comment"># plt.clf()</span>
        params = {'mathtext.default': 'regular' }  <span class="comment"># mathtext, for subscripting 0 in v0 in plot title</span>
        tmax = (2 * self.v) * np.sin(rad) / self.g <span class="comment"># calculate time of flight</span>
        t = tmax*np.linspace(0,1,100) <span class="comment"># divide time of flight into 100 uniform time steps</span>
        self.x = ((self.v * t) * np.cos(rad)) <span class="comment"># horizontal distance at each time step</span>
        self.y = ((self.v * t) * np.sin(rad)) - ((0.5 * self.g) * (t ** 2)) <span class="comment"># vertical distance </span>

        plt.plot(self.x, self.y, color='g') <span class="comment"># plot path</span>

        <span class="comment"># draw line to target target (i.e desired distance) saved in `self.target`</span>
        plt.axvline(x = self.target, ls='--', color = 'b', label = f'target: {self.target}m') 

        <span class="comment"># draw projectile at final coordinates</span>
        plt.scatter(self.x[-1], self.y[-1], color='r', marker="^", s=200)

        plt.ylim([0,10])
        plt.xlim(left=0)
        plt.title(f'$v_{0}$ = {self.v}m/s, Œ∏ = {self.angle}¬∞, abs. error = {np.abs(self.x[-1]-self.target):.2f}m')
        plt.legend()
    
    def __repr__(self):
        return f'''Projectile environment: 
                    Initial velocity: {self.v}m/s
                    Acceleration due to gravity: {self.g}m/s¬≤
                    Available angles in degrees:{list(self.action_to_angle.values())}'''
</pre>  

      <h2>Solving this environment using the <code>Q</code> class</h2>
      <p>Using the range formula provided earlier, we can expect our Q-learning algorithms to learn the closest
        angles.<br> The table below contains actual values of the ranges covered at the provided angles</p>
      <table>
        <thead>
          <tr>
            <th>angle(¬∞) </th>
            <th style="text-align: center;"> Range(m/s) </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>5¬∞ </td>
            <td style="text-align: center;"> 3.98682 </td>
          </tr>
          <tr>
            <td>10¬∞ </td>
            <td style="text-align: center;"> 7.8525 </td>
          </tr>
          <tr>
            <td>15¬∞ </td>
            <td style="text-align: center;"> 11.4796 </td>
          </tr>
          <tr>
            <td>20¬∞ </td>
            <td style="text-align: center;"> 14.7579 </td>
          </tr>
          <tr>
            <td>25¬∞ </td>
            <td style="text-align: center;"> 17.5878 </td>
          </tr>
          <tr>
            <td>30¬∞ </td>
            <td style="text-align: center;"> 19.8832 </td>
          </tr>
          <tr>
            <td>35¬∞ </td>
            <td style="text-align: center;"> 21.5746 </td>
          </tr>
          <tr>
            <td>40¬∞ </td>
            <td style="text-align: center;"> 22.6104 </td>
          </tr>
          <tr>
            <td>45¬∞ </td>
            <td style="text-align: center;"> 22.9592 </td>
          </tr>
        </tbody>
      </table>

      <p>Let's import the Q class defined in the previous article</p>
      <pre>from q_learning import Q</pre>
      <p>Let's train, applying the <code>Q</code> class to the <code>Projectile</code> environment</p>
<pre>
sim = Projectile()
sim.reset()
model = Q(sim, exploration_fraction=0.5)
model.train()
model.show(save_as='converged.png')
  </pre>
      <img src="../pics/Q table.png" alt="Q table after learning">
      <p>Great! Comparing the actions chosen by the algorithm after training to the correct values in the table above,
        we see we've been able to solve this toy problem using standard Q-Learning.</p>
    
  </div>
  </div>
</body>

</html>