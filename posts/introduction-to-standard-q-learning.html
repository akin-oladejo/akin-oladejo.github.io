<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CAN Intrusion Detection</title>
  <link rel="stylesheet" href="../styles.css">
</head>

<body class="layout title-page">
  <div class="left-nav">
    <div id="posts_home">
      <img src="icons/white arrow.png" class="arrow" alt="left arrow" id="arrow_p_home">
      <a href="../index.html">
        <div class="bflex" id="posts_home">POSTS</div>
      </a>
    </div>
    <p id="point">•</p>
    <div id="posts_about">
      <img src="icons/white arrow.png" class="arrow" alt="left arrow" id="arrow_p_about">
      <a href="../about.html">
        <div class="bflex">BIO</div>
      </a>
    </div>
  </div>

  <div class="post-body">
    <h1>Introduction to Standard Q-learning</h1>
    Reinforcement learning is the umbrella term for machine learning methods to learn by trial and error.

    In this paradigm, an agent receives two kinds of signals from the environment it is placed in:
    <ol>
      <li>the <b>state</b> (or in the partial case, <b>observation</b>) from that environment. This is an actionable
        representation of the environment; and</li>
      <li>the <b>reward</b> for actions it takes in that environment.</li>
    </ol>
    <p>It is safe to say that an action, in this context, is any manipulation from the agent in that environment that
      can change the state and request a reward value.</p>

    <p>The actions of an agent are determined by its internal rules, called the policy of the agent.</p>

    <p>The ultimate goal of the agent is to maximise the return (the total reward over a run or trajectory) and by so
      doing learn a task.</p>

    <p>Therefore, unlike supervised learning where a model is expected to predict an outcome given a labeled
      representation of similar incomes or unsupervised learning where a model makes <ins>inferences</ins> on data
      without labels, reinforcement learning is more bootstrapped and <ins>experimental</ins>.</p>

    <img src="../pics/agent-environment-loop.png" alt="agent-environment loop">

    Source: <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton & Barto</a>

    <p>As can be seen above, the agent takes an action and receives a reward and a new state. An effective policy
      determines the action it takes to increase the number of rewards it receives over time. Rewards can be negative,
      neutral or positive but the environment is designed so that the agent increases its rewards when it performs its
      task. The result of this system is an agent 'learning' the right behavior.</p>

    <h2>Definition</h2>
    There are a number of algorithms to train this agent but the focus for this post is the standard Q-learning
    algorithm.

    <blockquote>Standard Q-Learning is an off-policy, temporal-difference, value-based, tabular algorithm for solving
      reinforcement learning tasks.</blockquote>

    <p>Yikes! That feels like a long description but it helps with classifying algorithms. Let's unwind the thread by
      breaking down the terms:</p>

    <h4>1. Off-Policy</h4>
    To learn a task an agent needs to learn the value of different actions or states toward achieving its goal. It also
    needs to choose actions/states based on the values it has learned. The policy it uses to try states or state-action
    pairs in order to quantify them is called its behavior or updating policy because it is used in training. The policy
    it uses to act is called its target or acting policy. Some algorithms, like sarsa, use the same policy for both
    acting and updating. Q-learning, however, uses a separate policy to learn from the one it uses to select actions.

    <h4>2. Temporal-difference</h4>
    Think of the approach to learn the value of the current state/action as a spectrum. One extreme end is the algorithm
    learning the value of the current state/action by observing the reward in the next state. The other end of the
    spectrum is learning the long-term value of the current state by considering all the following states if you follow
    the acting policy. Observe the figure below:

    <img src="../pics/temporal difference chart.png" alt="temporal difference">

    <b>One-step temporal difference</b> is the left end. In temporal difference methods like Q learning, sarsa, expected
    sarsa and n-step TD, the agent's internal valuation of a state/action is determined by how much choosing this
    state/action influences reward collection over the next couple states or state-action pairs. It may seem daunting
    but the code is easy and we'll get to it in a bit.

    <p>The right end summarizes <b>Monte Carlo</b> methods. Monte Carlo methods value a state/action by exhaustion. This
      means that, to quantify the value of the current state, the agent will average the returns starting from that
      state or state-action pair till the end of the task (if it is episodic).</p>

    <p>One distinct implication of choosing Monte Carlo methods is that, a state or state-action pair may not be judged
      useful if the returns at the end of the episode (when you choose that state or state-action pair) are little
      compared to sibling states or state-action pairs. However, under a TD algorithm, if only the immediate state
      after yields a high return, that state or state-action pair will be valued higher.</p>

    <p>In between both ends on the pictured spectrum is the n-step TD. You can already guess why they're called so —
      they look more than one step ahead, but not till the end of the episode. They combine the strengths of both TD
      and Monte Carlo methods and try to avoid some pitfalls either kind causes.</p>

    <h4>3. Value-based</h4>
    <p>As opposed to policy-based reinforcement learning algorithms, value-based algorithms do not learn a policy
      explicitly. Instead, they learn a value function from which we can derive a policy.</p>

    <h4>4. Tabular</h4>
    <p>Standard Q-Learning uses a 'lookup' table to guide its action-making. The updating policy populates this table,
      called the Q-table and once training is complete, uses the acting policy to determine actions given a state. The
      acting policy uses the greedy approach to select actions i.e it selects the action with the highest q-value for
      the current state.</p>

    <h2>Understanding a Few More Concepts</h2>
    <p>Before we get into the code, there are a couple more concepts to assimilate, that will hopefully seed the
      concepts above in concrete:</p>

    <h3>The Exploration-exploitation tradeoff:</h3>
    <p>Here's an analogy: there's a fried chicken spot on the way to work and the food's just okay. You don't
      necessarily lick the paper plates you get served in but you can't complain. Now, a colleague puts you on to an
      Amala spot a little out of the way. 'It's amala,' you think, 'if it's any good, it'll be leagues above chicken
      in a paper plate.'</p>

    <p>>Everyday you go to the chicken place, knowing it is better than your current options, you are said to be
      exploiting.
      If you put in some effort to try the amala spot, you may discover it offers better value for your money and time
      (but there's also the chance this is false). Navigating to the amala spot is called exploring. This concept is
      useful in many areas, like reinforcement learning.</p>

    <p>During training, to avoid the pitfall of learning the quality of one state-action pair and neglecting (possibly
      better) alternatives, it is useful to explore and come by a fair score of as many actions as possible. One way to
      do
      this is the epsilon-greedy policy (written <code>ε</code>-greedy):

    <blockquote>Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no
      time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is
      to behave greedily most of the time, but every once in a while, say with small probability ε, instead select
      randomly from among all the actions with equal probability, independently of the action-value estimates. We call
      methods using this near-greedy action selection rule ε-greedy methods (Sutton & Barto)</blockquote>

    Following the analogy described earlier, if your decision to go to a shop depends on the results of a coin flip, the
    value of epsilon would, theoretically, be 0.5. At ε=0.5, half the time, you'll go to the chicken spot you're sure
    of. The other half of the time, you pick a place at random so you may eventually pick the amala spot and see if
    that's better. So that your decision is random, you can have your pet, or someone else's pet make the decision for
    you somehow!</p>

    <h3>Epsilon Decay</h3>
    As we progress through training, we want to explore less. Why? Having built some intuition around the action-values,
    we want to reduce the randomness of action selection in order to get closer to convergence.
    While there are a couple ways to decrease the value of <code>ε</code> over time (this is called annealing), in the
    code below I used a linear decay scheme: up until a timestep which will be calculated using the exploration fraction
    (an argument provided by the user), <code>ε</code> decrements uniformly from a maximum value to a minimum value
    (both also passed as arguments). After this timestep, <code>ε</code> constantly remains at its minimum value,
    reducing the rate of exploration but not stopping it.

    <h3>The Q-learning Algorithm</h3>
    <img src="../pics/q learning pseudocode.png" alt="Q-Learning pseudocode">

    The training loop for the standard q-learning algorithm is quite simple:
    <ol>
      <li>Initialize the learning rate, <code>α</code>.<br>
        This value determines by how much updates to an action-value should be made. If the value of <code>α</code>
        is too large, updates could take giant skips and miss the optimal values, thus never converging. If the
        value is too small, the action-values will take too long to converge. <b>Convergence</b> occurs when the
        algorithm is no longer learning.</li>
      <li>Initialize the q-table arbitrarily<br>
        In theory, q-learning has been proven to converge to optimal action-values. We can therefore initialize our
        q-table to arbitrary values. <b>Training</b> occurs when we update this table.</li>
      <li>For each episode of the training loop:
        <ol class="a">
          <li>initialize the starting state</li>
          <li>For each step of the current episode:
            <ol class="i">
              <li>use the acting policy to select an action (i.e the ε-greedy policy)</li>
              <li>record the reward obtained and the next state value</li>
              <li>update the q-value of the current state using the formula:
                <img src="../pics/q learning formula.png" alt="Q-Learning Formula"></img>
              </li>
            </ol>
          <li>update the state to the new state</li>
      </li>
    </ol>
    <h3>Understanding the update rule</h3>
    Update rules are usually of the form:
    <img src="../pics/q update pseudocode.png" alt="Q-Learning Update rule">

    Step 3b(ii) above is q-learning's version of the update rule, where the magic happens. Let's understand how it
    happens, starting with the values in the parentheses.

    To evaluate the q-value of a state-action pair (perhaps now is a good time to state that 'q' stands for the
    <ins>quality</ins> of a state action pair), the algorithm evaluates the <b>TD target</b> (which was described
    earlier to be the one-step return):
    <pre>TD target = <img src="../pics/td target.png" alt="TD Target" class="inline"></pre>
    <br>
    which basically means the current reward plus the discounted q-value of the a state-action pair for the immediate
    next state selected using the <ins>greedy</ins> method. It is because we use the greedy approach for the
    <b>updating</b> (as opposed to epsilon-greedy approach we used for <b>acting</b>) that q-learning is called an
    <ins>off-policy</ins> algorithm.

    <p>Having computed the one-step return, we need to know the direction we should change the current action-value:
      whether to increase or decrease it. Therefore, we subtract the old action-value from the newly computed TD
      target. This difference is called the <b>TD error</b>:</p>

      <img src="../pics/td error.png" alt="TD Error">

      <p>The value of TD error tells us whether to increase or decrease the action-value. If TD error is positive, the
      update step increases the action-value, otherwise it decreases it.
    </p>
    <p>
      The next thing is the step size, what we called learning rate earlier, or <code>α</code> (alpha). <code>α</code>
      tells us by how much we should change the current action value, given that we know what direction we should
      change it. As stated earlier, it is important to choose a value that is not so large that we always skip the
      optimal value and not so small that it takes forever to converge.
    </p>

    <p>
      Therefore, the update to the current action value will be a proportionate increase or decrease, allowing us to
      be able to change the action-value over the training loop.
    </p>

    <h2>Implementing the Q-learning algorithm in code</h2>
    <p>Our q-learning algorithm will be implemented as a class <code>Q</code>, to tie things together.</p>

    <h3>Initialization</h3>
    The <code>__init__</code> method (magic method used to customize an object) is called when an object is
    instantiated. The Q object takes in a couple hyperparameter arguments:
    <ul>
      <li><code>env</code> an environment of type gym.Env (the environment should subclass the Env class provided in
        the openai gym library, for standard use). In the next article, we will create a custom environment to solve
        using this q-learning algorithm.</li>
      <li><code>min_epsilon</code>: a float value that will be the value of ε after annealing,</li>
      <li><code>min_epsilon</code>: a float value that will be the value of ε before annealing,</li>
      <li><code>exploration_fraction</code>: the fraction of the training timesteps over which to anneal ε,</li>
      <li><code>lr</code>: a float value representing the step size/learning rate,</li>
      <li><code>gamma</code>: a float value (denoted <code>γ</code>) representing the discount rate</li>
    </ul>

<pre>
def __init__(self, 
              env:Env,
              min_epsilon:float = 0.02,
              max_epsilon:float = 0.5,
              exploration_fraction:float = 0.4,
              lr:float = 0.7,
              gamma:float = 0.95):
    
    <span class="comment"># store the environment</span>
    self.env = env

    <span class="comment"># create table</span>
    self.qtable = np.zeros(shape=(self.env.n_states, self.env.n_actions))

    <span class="comment"># set hyperparameters</span>
    self.lr = lr
    self.gamma = gamma
    self.min_epsilon = min_epsilon
    self.max_epsilon = max_epsilon
    self.exploration_fraction = exploration_fraction
</pre>

    <h3><code>greedy_policy</code> and <code>epsilon_greedy_policy</code></h3>
    The logic for exploitation and exploration is written in both methods:

<pre>
def greedy_policy(self):
    <span class="comment">'''Choose the action with the highest q-value for that state'''</span>
    return np.argmax(self.qtable[self.state])
        
def epsilon_greedy_policy(self, epsilon):
    <span class="comment">'''Choose any action sometimes, other times choose the one with high q-value'''</span>

    <span class="comment"># generate probability value to match epsilon against</span>
    rando = random.uniform(0,1)

    <span class="comment"># if value < epsilon, explore (because we will be decaying epsilon gradually)</span>
    if rando < epsilon:
        action = self.env.action_space.sample()
    <span class="comment"># else, exploit</span>
    else:
        action = np.argmax(self.qtable[self.state])
    
    return action
</pre>

    <h3>The <code>train</code> method</h3>
    The <code>train</code> method takes a single argument: the hyperparameter <code>train_episodes</code> which is the
    number of episodes to learn a given task.
    The training follows this logic:
    <ol>
      <li>Determine the number of timestamps to anneal <code>ε</code> using <code>exploration_fraction</code> and
        <code>train_episodes</code>. Store this value as <code>n_exploration_timesteps</code>
      </li>
      <li>Create an array epsilon_values to contain evenly-spaced values of epsilon over
        <code>n_exploration_timesteps</code>. The array begins with <code>max_epsilon</code> and ends with
        <code>min_epsilon</code>
      </li>
      <li>For each episode, run the following loop:
        <ol class="i">
          <li>Evaluate epsilon. If n_exploration_timesteps has not been existed, supply the next element of the
            epsilon_values array. Else, set epsilon to min_epsilon</li>
          <li>reset the environment and store the starting state</li>
          <li>while the episode is not done, supply an action using the acting policy, receive rewards and a new
            state and perform qtable updates with these values using the updating policy</li>
          <li>move to the new state and repeat</li>
        </ol>
      </li>
    </ol>

    <pre class="py">
def train(self, train_episodes:int = 5000):     
    <span class="comment"># calculate the number of timesteps to anneal exploration</span>
    self.n_exploration_timesteps = int(self.exploration_fraction * train_episodes )
    self.epsilon_values = np.linspace(self.max_epsilon, self.min_epsilon, self.n_exploration_timesteps)

    for episode in tqdm(range(train_episodes)):
        <span class="comment"># adjust epsilon</span>
        if episode < self.n_exploration_timesteps:
            self.epsilon = self.epsilon_values[episode]
        else:
            self.epsilon = self.min_epsilon

        self.state, _ = self.env.reset(verbose=False) <span class="comment"># reset the episode</span>
        
        #train loop
        while not self.env.done:
            <span class="comment"># print(self.state)</span>
            if episode < self.n_exploration_timesteps:
                self.action = self.env.action_space.sample()
            else:
                self.action = self.greedy_policy()

            <span class="comment"># take an ε-greedy action</span>
            <span class="comment"># self.action = self.epsilon_greedy_policy(self.epsilon)</span> 
            self.new_state, reward, done, info = self.env.step(self.action)
            
            <span class="comment"># update self.qtable using Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]</span>
            self.td_target = reward + self.gamma*np.max(self.qtable[self.new_state]) <span class="comment"># compute the one-step return</span>
            self.td_error = self.td_target - self.qtable[self.state][self.action]
            self.qtable[self.state][self.action] = self.qtable[self.state][self.action] + (self.lr * self.td_error)

            <span class="comment"># Render the env</span>
            <span class="comment"># self.env.render()</span>
            <span class="comment"># Wait a bit before the next frame </span>
            <span class="comment"># sleep(0.001)</span>

            <span class="comment"># if done, terminate</span>
            if done:
                break

            self.state = self.new_state
</pre>
    <h3>Visualizing the Q-Table</h3>
    Below is the logic to visualize the q-table using seaborn a visualization library
    <pre class="py">
def show(self):
    <span class="comment"># plot the qtable as a heatmap</span>
    plt.figure(figsize=(10,5))
    plt.title(f'Qtable for {self.env.name} environment')
    ax = sns.heatmap(self.qtable, annot=True, linecolor='pink', linewidths=0.2, cmap='Greys')
    ax.set(xlabel='angle(°)', ylabel='target distance(m)') <span class="comment"># label axes</span>
    plt.xticks(ticks=np.arange(self.env.n_actions)+0.5, labels=self.env.action_to_angle.values(), ha='center') <span class="comment">change xticks to angles</span>
    state_to_target = {j:i for i,j in self.env.target_to_state.items()}
    plt.yticks(ticks=np.arange(self.env.n_states)+0.5, labels=state_to_target.values(), va='center')
</pre>

    <h2>Putting it all together: the Q class</h2>
    <pre>
class Q():
  def __init__(self, 
                env:Env,
                min_epsilon:float = 0.02,
                max_epsilon:float = 0.5,
                exploration_fraction:float = 0.4,
                lr:float = 0.003,
                gamma:float = 1.0):
      
    <span class="comment"># store the environment</span>
    self.env = env

    <span class="comment"># create table</span>
    self.qtable = np.zeros(shape=(self.env.n_states, self.env.n_actions))

    <span class="comment"># set hyperparameters</span>
    self.lr = lr
    self.gamma = gamma
    self.min_epsilon = min_epsilon
    self.max_epsilon = max_epsilon
    self.exploration_fraction = exploration_fraction
      
      

  def greedy_policy(self):
      <span class="comment">'''Choose the action with the highest q-value for that state'''</span>
      return np.argmax(self.qtable[self.state])


  def epsilon_greedy_policy(self, epsilon):
      '''
      Choose any action sometimes, other times choose the one with high q-value
      '''
      <span class="comment"># generate probability value to match epsilon against</span>
      rando = random.uniform(0,1)

      <span class="comment"># if value < epsilon, explore (because we will be decaying epsilon gradually)</span>
      if rando < epsilon:
          action = self.env.action_space.sample()
      <span class="comment"># else, exploit</span>
      else:
          action = np.argmax(self.qtable[self.state])
      
      return action

  def train(self, train_episodes:int = 5000): 
      
      <span class="comment"># calculate the number of timesteps to anneal exploration</span>
      self.n_exploration_timesteps = int(self.exploration_fraction * train_episodes )
      self.epsilon_values = np.linspace(self.max_epsilon, self.min_epsilon, self.n_exploration_timesteps)

      for episode in tqdm(range(train_episodes)):
          <span class="comment"># adjust epsilon</span>
          <span class="comment"># self.epsilon = self.min_epsilon + (self.max_epsilon-self.min_epsilon)*np.exp(-self.decay_rate*episode)</span>
          if episode < self.n_exploration_timesteps:
              self.epsilon = self.epsilon_values[episode]
          else:
              self.epsilon = self.min_epsilon

          self.state, _ = self.env.reset(verbose=False) <span class="comment"># reset the episode</span>
          
          #train loop
          while not self.env.done:
              <span class="comment"># print(self.state)</span>
              if episode < self.n_exploration_timesteps:
                  self.action = self.env.action_space.sample()
              else:
                  self.action = self.greedy_policy()

              <span class="comment"># take an ε-greedy action</span>
              <span class="comment"># self.action = self.epsilon_greedy_policy(self.epsilon)</span>
              self.new_state, reward, done, info = self.env.step(self.action)
              
              <span class="comment"># update self.qtable using Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]</span>
              self.td_target = reward + self.gamma*np.max(self.qtable[self.new_state]) <span class="comment"># compute the one-step return</span>
              self.td_error = self.td_target - self.qtable[self.state][self.action]
              self.qtable[self.state][self.action] = self.qtable[self.state][self.action] + (self.lr * self.td_error)

              <span class="comment"># Render the env</span>
              <span class="comment"># self.env.render()</span>
              <span class="comment"># Wait a bit before the next frame </span>
              <span class="comment"># sleep(0.001)</span>

              <span class="comment"># if done, terminate</span>
              if done:
                  break

              self.state = self.new_state


  def __repr__(self):
      return f"Run the 'Q.show()' method to show the Qtable better\n{self.qtable}"
      
  def show(self, save_as:str):
      <span class="comment"># plot the qtable as a heatmap</span>
      plt.figure(figsize=(10,5))
      plt.title(f'Qtable for {self.env.name} environment')
      ax = sns.heatmap(self.qtable, annot=True, linecolor='pink', linewidths=0.2, cmap='Greys')
      ax.set(xlabel='angle(°)', ylabel='target distance(m)') <span class="comment"># label axes</span>
      plt.xticks(ticks=np.arange(self.env.n_actions)+0.5, labels=self.env.action_to_angle.values(), ha='center') <span class="comment"># change xticks to angles</span>
      state_to_target = {j:i for i,j in self.env.target_to_state.items()}
      plt.yticks(ticks=np.arange(self.env.n_states)+0.5, labels=state_to_target.values(), va='center')
      
      <span class="comment"># save image to file</span>
      if save_as:
          plt.savefig(save_as)
</pre>

    <h2>Conclusion</h2>
    For an extensive explanation of Q-learning and other algorithms , you can refer to any of the following resources:
    <ol>
      <li><a href="https://github.com/huggingface/deep-rl-class">The Huggingface course on reinforcement learning</a>
        (This is good. I am currently taking it at the time of writing this)</li>
      <li><a href="http://incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a>. A
        book by Richard S. Sutton and Andrew G. Barto (This is an excellent reinforcement learning theory material.
        At the time of writing this, I use this material to supplement my learning efforts).</li>
      <li><a href="https://youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm">The Deepmind Reinforcement
          Learning Youtube playlist</a>.</li>
    </ol>
    We will apply the Q class to a custom environment in the next article. See you then.

  </div>
</body>

</html>